{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importowanie potrzebnych bibliotek startowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baza Danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'database/text_and_tags_small.tsv'\n",
    "dataset = pd.read_csv(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oczyszczanie tekstów artykułów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- w pierwszej kolejności usuwamy znaki, niebędące literami oraz zmieniamy wszystkie litery na małe\n",
    "- kazdy artykuł dzielony jest następnie na liste słów\n",
    "- przeprowadzamy stemming na słowach, niebędących tzw. \"stop words\"\n",
    "- listę ze zmianami łączymy z powrotem w artykuł\n",
    "\n",
    "Czym jest stemming? \n",
    "- jest to proces przekształcania słów do ich formy podstawowej np. \"running\" do formy \"run\"\n",
    "\n",
    "Czym są \"stop words\"?\n",
    "- są to słowa często pojawiające się w tekstach, ale nie wnoszą do nich niczego co pomaga w analizie\n",
    "- np. \"the\", \"is\", \"at\" itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/minis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re # importujemy bibliotekę do obsługi wyrażeń regularnych\n",
    "import nltk # importujemy bibliotekę do przetwarzania języka naturalnego\n",
    "nltk.download('stopwords') # pobieramy listę \"stop words\" z biblioteki nltk\n",
    "from nltk.corpus import stopwords # importujemy \"stop words\" z nltk.corpus\n",
    "from nltk.stem.porter import PorterStemmer # importujemy PorterStemmer do stemmingu\n",
    "\n",
    "corpus = [] # inicjalizujemy pusty korpus\n",
    "for i in range(0, len(dataset)): # iterujemy przez wszystkie artykuły\n",
    "    article = re.sub('[^a-zA-Z]', ' ', dataset['text'][i]) # usuwamy wszystkie znaki, które nie są literami\n",
    "    article = article.lower() # zamieniamy wszystkie litery na małe\n",
    "    article = article.split() # dzielimy artykuł na listę słów\n",
    "\n",
    "    ps = PorterStemmer() # inicjalizujemy stemmer\n",
    "    article = [ps.stem(word) for word in article if not word in set(stopwords.words('english'))] # usuwamy \"stop words\" i stosujemy stemming\n",
    "    article = ' '.join(article) # łączymy słowa z powrotem w artykuł\n",
    "    corpus.append(article) # dodajemy przetworzony artykuł do korpusu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wektoryzacja tekstu\n",
    "\n",
    "W tym bloku kodu używamy `CountVectorizer` z biblioteki `scikit-learn`, aby przekształcić nasz przetworzony tekst (korpus) w wektory liczbowe, które mogą być użyte do trenowania modeli uczenia maszynowego. `CountVectorizer` konwertuje kolekcję dokumentów tekstowych na macierz liczb całkowitych, reprezentującą liczbę wystąpień słów. Oto co robi każda linia kodu:\n",
    "\n",
    "- `from sklearn.feature_extraction.text import CountVectorizer`: Importujemy klasę `CountVectorizer`.\n",
    "- `cv = CountVectorizer(max_features=1500)`: Tworzymy instancję `CountVectorizer`, ograniczając liczbę cech do 1500. Oznacza to, że wektor będzie zawierał 1500 najczęściej występujących słów w korpusie.\n",
    "- `X = cv.fit_transform(corpus).toarray()`: Dopasowujemy `CountVectorizer` do naszego korpusu i transformujemy tekst na wektory liczbowe. Metoda `toarray()` konwertuje wynik na tablicę numpy, co jest przydatne dla wielu algorytmów uczenia maszynowego, które oczekują danych wejściowych w tej formie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowywanie tagów\n",
    "\n",
    "W tej części kodu wykorzystujemy `MultiLabelBinarizer` z biblioteki `scikit-learn`, aby przekształcić etykiety tekstowe w postaci tagów na binarne wektory. Jest to niezbędne w problemach klasyfikacji wieloetykietowej, gdzie każda próbka może być przypisana do wielu klas jednocześnie. Oto szczegółowy opis działania tego fragmentu:\n",
    "\n",
    "- `from sklearn.preprocessing import MultiLabelBinarizer`: Importujemy klasę `MultiLabelBinarizer`.\n",
    "- `mlb = MultiLabelBinarizer()`: Tworzymy instancję `MultiLabelBinarizer`.\n",
    "- `y = mlb.fit_transform(dataset['tags'])`: Dopasowujemy `MultiLabelBinarizer` do kolumny 'tags' naszego zbioru danych i transformujemy listę tagów do binarnej formy macierzy. Każda kolumna w tej macierzy odpowiada jednemu tagowi, a wartość 1 oznacza, że tag jest przypisany do danej próbki, natomiast 0 oznacza jego brak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['tags'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktualnie tagi są listą zawierającą pojedyncze elementy, które są ciągami znaków. Musimy przekształcić ten fromat na listę list, gdzie kazda wewnętrzna lista zawiera etykiety jako odzielne ciągi znaków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Mental Health, Health, Psychology, Science, N...\n",
      "1    [Mental Health, Coronavirus, Science, Psycholo...\n",
      "Name: tags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def transform_tags(tag_string):\n",
    "    try:\n",
    "        return ast.literal_eval(tag_string)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "dataset['tags'] = dataset['tags'].apply(transform_tags)\n",
    "print(dataset['tags'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(dataset['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział na zbiory testowe i treningowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nauka modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten blok kodu jest częścią procesu uczenia maszynowego, w którym tworzony i trenowany jest model klasyfikacji wieloetykietowej za pomocą algorytmu Naive Bayes. Oto szczegółowy opis:\n",
    "\n",
    "- from sklearn.naive_bayes import MultinomialNB: Importuje klasę MultinomialNB z biblioteki scikit-learn. MultinomialNB to implementacja naiwnego klasyfikatora Bayesa dla rozkładu wielomianowego, który jest odpowiedni dla danych z cechami dyskretnymi (np. liczba wystąpień słowa w tekście).\n",
    "\n",
    "- from sklearn.multioutput import MultiOutputClassifier: Importuje klasę MultiOutputClassifier, która jest strategią do rozszerzenia klasyfikatorów jednoetykietowych na klasyfikatory wieloetykietowe. Pozwala to na przewidywanie wielu zależnych zmiennych kategorialnych (etykiet).\n",
    "\n",
    "- model = MultiOutputClassifier(MultinomialNB()): Tworzy instancję MultiOutputClassifier, przekazując do niej instancję MultinomialNB jako estymator bazowy. To oznacza, że dla każdej etykiety (kolumny w y_train) zostanie utworzony osobny klasyfikator MultinomialNB.\n",
    "\n",
    "- model.fit(X_train, y_train): Metoda fit jest używana do trenowania modelu na podstawie danych treningowych. X_train zawiera wektory cech (przetworzone teksty), a y_train zawiera odpowiadające im etykiety w formie binarnej. Model uczy się, jak przewidywać etykiety na podstawie cech wejściowych.\n",
    "\n",
    "Po wykonaniu tego kodu, model jest wytrenowanym klasyfikatorem wieloetykietowym, który może być używany do przewidywania etykiet dla nowych, nieznanych próbek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Import klasy MultinomialNB z biblioteki scikit-learn\n",
    "# MultinomialNB to implementacja naiwnego klasyfikatora Bayesa dla rozkładu wielomianowego\n",
    "model = MultiOutputClassifier(MultinomialNB())\n",
    "\n",
    "# Utworzenie modelu klasyfikacji wieloetykietowej\n",
    "# MultiOutputClassifier pozwala na rozszerzenie klasyfikatorów jednoetykietowych\n",
    "# na klasyfikatory wieloetykietowe\n",
    "model.fit(X_train, y_train)  # Trenowanie modelu na podstawie danych treningowych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten blok kodu służy do przewidywania etykiet dla zestawu testowego (`X_test`) przy użyciu wcześniej wytrenowanego modelu klasyfikacji wieloetykietowej. Wywołanie metody `predict` na obiekcie `model` powoduje, że model stosuje nauczony algorytm do nowych danych i generuje przewidywane etykiety (`y_pred`).\n",
    "\n",
    "- `y_pred = model.predict(X_test)`: Przewiduje etykiety dla danych testowych `X_test` wykorzystując wytrenowany model. Wynik przypisany do zmiennej `y_pred` zawiera przewidziane etykiety w formie binarnej, gdzie każda kolumna odpowiada jednej etykiecie, a wartość 1 oznacza przewidzienie obecności etykiety, a 0 jej brak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Loss\n",
    "-  **Opis:** Hamming Loss to miara błędu używana w klasyfikacji wieloetykietowej. Określa średnią liczbę etykiet, które są źle przewidywane, tj. niewłaściwa etykieta jest przewidywana jako obecna lub właściwa etykieta jest przewidywana jako nieobecna.\n",
    "-  **Sposób interpretacji:** Im niższa wartość Hamming Loss, tym lepszy jest model. Wartość 0 oznacza, że nie ma błędów w przewidywaniu etykiet. Wartość bliższa 1 wskazuje na większą liczbę błędów. W kontekście klasyfikacji wieloetykietowej, nawet niewielkie wartości Hamming Loss mogą być uważane za dobre wyniki, biorąc pod uwagę złożoność zadania.\n",
    "\n",
    "### Precyzja\n",
    "-  **Opis:** Precyzja (precision) to miara, która określa, jaki procent etykiet przewidzianych jako pozytywne przez model jest faktycznie pozytywny. Innymi słowy, jest to stosunek prawidłowo przewidzianych pozytywnych instancji do ogólnej liczby instancji przewidzianych jako pozytywne.\n",
    "-  **Sposób interpretacji:** Wysoka precyzja oznacza, że model rzadko oznacza negatywną etykietę jako pozytywną. Niska precyzja wskazuje, że model często błędnie klasyfikuje negatywne etykiety jako pozytywne. W idealnym przypadku precyzja powinna być jak najbliższa 1.\n",
    "\n",
    "### Pełność (Recall)\n",
    "-  **Opis:** Pełność (recall) to miara zdolności modelu do znalezienia wszystkich pozytywnych instancji. Określa, jaki procent rzeczywistych pozytywnych etykiet został poprawnie zidentyfikowany przez model.\n",
    "-  **Sposób interpretacji:** Wysoka pełność oznacza, że model dobrze radzi sobie z identyfikacją pozytywnych etykiet, nawet kosztem zwiększenia liczby fałszywie pozytywnych przewidywań. Niska pełność wskazuje, że model przegapił wiele pozytywnych etykiet. Idealna pełność to wartość 1, co oznacza, że wszystkie pozytywne etykiety zostały poprawnie przewidziane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, hamming_loss\n",
    "\n",
    "# Hamming Loss\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "# Oblicz precyzję\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "# Oblicz pełność (recall)\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Hamming Loss: \", hamming)\n",
    "print(\"Precyzja: \", precision)\n",
    "print(\"Pełność: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksport gotowego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(model, 'model_100k/model.joblib')\n",
    "dump(cv, 'model_100k/count_vectorizer.joblib')\n",
    "dump(mlb, 'model_100k/multilabel_binarizer.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
